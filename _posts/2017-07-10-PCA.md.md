PCA 理解



---
layout: post
title: 'PCA'
subtitle: '15331404 张益强'
date: 2017-08-10
categories: tensorflow datamining
cover: 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSJmUnN6ur0wCWy1Nb_U_v4GN4CbryYxmmRA7G5QDMdeZSP0-v0'
tags: datamining tensorflow
---



tags: tensorflow datamining
---

- PCA降维的大致思想就是： 挑选特征明显的、显得比较重要的信息保留下来。

- 降维：用更少的成本描述更多的信息。一方面是可以降低信噪比，让数据更容易被学习，其实就是对数据进行了一次去冗，另外也是为了性能，毕竟维度太高是算不动的

 - 选择标准有两个：1： 同一个维度内的数据，方差大的比较明显，因为方差大表示自己和平均水平差异大，有个性，降维后也最可能分的开～2： 两个不同维度间关联度越小越好，因为关联度小表示这两个维度表征共同信息的量比较少，最理想就是两个维度不相关，相关度为0（相关度可以用协方差cov(a,b)表示），在线性空间内表现为两个维度正交～

   

- 把SVD(奇异值分解)看作是对非方阵做PCA处理的一种方式啦，毕竟两者的套路都差不多，分解出特征值（SVD里是奇异值，数据XX‘的特征值的平方根），挑比较大的特征值对应的特征向量构成投影矩阵，然后做线性变换（将数据X投影到低维空间）


参考链接
[xx](http://blog.codinglabs.org/articles/pca-tutorial.html)