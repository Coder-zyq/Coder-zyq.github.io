---
layout: post

title: 'PCA'

subtitle: '15331404 张益强'

date: 2017-08-10

categories: tensorflow datamining

cover: 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSJmUnN6ur0wCWy1Nb_U_v4GN4CbryYxmmRA7G5QDMdeZSP0-v0'

tags: datamining tensorflow
---

## PCA 理解

- PCA降维的大致思想就是： 挑选特征明显的、显得比较重要的信息保留下来。

- 降维：用更少的成本描述更多的信息。一方面是可以降低信噪比，让数据更容易被学习，其实就是对数据进行了一次去冗，另外也是为了性能，毕竟维度太高是算不动的

 - 选择标准有两个：1：(基于最大投影方差) 同一个维度内的数据，方差大的比较明显，因为方差大表示自己和平均水平差异大，有个性，降维后也最可能分的开～2： 两个不同维度间关联度越小越好，因为关联度小表示这两个维度表征共同信息的量比较少，最理想就是两个维度不相关，相关度为0（相关度可以用协方差cov(a,b)表示），在线性空间内表现为两个维度正交～

   

- 把SVD(奇异值分解)看作是对非方阵做PCA处理的一种方式啦，毕竟两者的套路都差不多，分解出特征值（SVD里是奇异值，数据XX‘的特征值的平方根），挑比较大的特征值对应的特征向量构成投影矩阵，然后做线性变换（将数据X投影到低维空间）



若AX=λX，则称λ是A的特征值，X是对应的特征向量。实际上可以这样理解：矩阵A作用在它的特征向量X上，仅仅使得X的长度发生了变化，缩放比例就是相应的特征值λ。 



## PCA过程

1. 特征中心化，每一维数据减去该维度下的均值，得到矩阵A

2. 计算协方差矩阵B

3. 计算协方差矩阵B的特征值和特征向量

4. 选取大的特征值对应的特征向量，得到最新的数据集

   $ A‘_{m*b} =A_{m*a }M_{a*b}$

   其中M为求得的特征向量。

   





参考链接
[xx](http://blog.codinglabs.org/articles/pca-tutorial.html)

[主成分分析PCA](http://www.cnblogs.com/zhangchaoyang/articles/2222048.html)

