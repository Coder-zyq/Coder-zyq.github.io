---
layout: post
title: '归一化，标准化，and OneHot'
subtitle: '15331404 张益强'
date: 2018-05-10
categories: tensorflow datamining
cover: 'http://img.article.pchome.net/00/34/54/50/pic_lib/wm/Black04.jpg'
tags: datamining tensorflow
---

### Standardization vs. normalization

[参考链接](https://blog.csdn.net/pipisorry/article/details/52247379)

https://blog.csdn.net/sallyxyl1993/article/details/69364181

http://www.dataminingblog.com/standardization-vs-normalization/

#### Standardization(规范化)：

It will then transform it to have zero mean and unit variance 

![](http://3.bp.blogspot.com/_xqXlcaQiGRk/RpO4yR0oKtI/AAAAAAAAABM/7rUWCMwizus/s1600/fig2.png)

#### normalization（标准化）：





![](http://3.bp.blogspot.com/_xqXlcaQiGRk/RpO4CR0oKqI/AAAAAAAAAA0/TnshqtR_ndw/s1600/fig1.png)



#### 标准化

数据的标准化（normalization）是将数据按比例缩放，使之落入一个小的特定区间。在某些比较和评价的指标处理中经常会用到，去除数据的单位限制，将其转化为无量纲的纯数值，便于不同单位或量级的指标能够进行比较和加权。其中最典型的就是数据的归一化处理，即将数据统一映射到[0,1]区间上。  

#### 标准化的方法

1. 最小－最大规范化（线性变换）  

2. z-score规范化（或零－均值规范化） 

3. 小数定标规范化：通过移动X的小数位置来进行规范化  　　 y= x/(10^j)　（其中，j为使得Max(|y|) <1的最小整数）  ,例如 假定A的值由-986到917，A的最大绝对值为986，为使用小数定标标准化，我们用每个值除以1000（即，j=3），这样，-986被规范化为-0.986 

4.  对数Logistic模式(**softmax**)：  　　 新数据=1/(1+e^(-原数据))  

5.  模糊量化模式：  　　 新数据=1/2+1/2sin[pi/（极大值-极小值）*(X-(极大值-极小值)/2)]  

6.  batch normalization 

   ```c
   什么时候Min——max scaling好，什么时候z-zero standardization 好呢
   1、在分类、聚类算法中，需要使用距离来度量相似性的时候、或者使用PCA技术进行降维的时候，第二种方法(Z-score standardization)表现更好。
   2、在不涉及距离度量、协方差计算、数据不符合正太分布的时候，可以使用第一种方法或其他归一化方法。比如图像处理中，将RGB图像转换为灰度图像后将其值限定在[0 255]的范围。
   ```

   https://blog.csdn.net/zbc1090549839/article/details/44103801

#### 归一化

> 好处：1.提升模型收敛速度 2.提升模型精度（可以让各个特征对结果做出的贡献相同 ）



![](http://images2015.cnblogs.com/blog/743682/201511/743682-20151108152327539-2039269197.png)





1. 把数变为（0，1）之间的小数 

主要是为了数据处理方便提出来的，把数据映射到0～1范围之内处理，更加便捷快速，应该归到数字信号处理范畴之内。 

2. 把有量纲表达式变为无量纲表达式 

归一化是一种简化计算的方式，即将有量纲的表达式，经过变换，化为无量纲的表达式，成为纯量。 比如，复数阻抗可以归一化书写：Z = R + jωL = R(1 + jωL/R) ，复数部分变成了纯数量了，没有量纲。  另外，微波之中也就是电路分析、信号系统、电磁波传输等，有很多运算都可以如此处理，既保证了运算的便捷，又能凸现出物理量的本质含义。 

### OneHot

[参考链接](https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/)



> A one hot encoding is a representation of categorical variables as binary vectors. (将分类变量变为二进制向量)
>
> [0,1,2,3]表示为[ [1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1] ]

#### why use OneHot

1. A one hot encoding allows the representation of categorical data to be more expressive. （使分类的数据更具表现力，for machine）
2. use an integer encoding directly, rescaled where needed. （直接使用整数编码，扩展容易）
3. 欧式空间距离，{1，2，3}，1，3距离2，其实不是的，分类1和分类三只是距离一个欧式距离。

#### how we use

```python
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
import numpy as np

#input[size,1] output[size, n_class]
OneHot_encoder = OneHotEncoder(sparse = False)
OneHot_Y = OneHot_encoder.fit_transform(input)
# notice the relationship [1,2] ==>[[1,0],[0,1]]
```

##### Label_EnCoder(将字符数组，表示为数字数组)

```python
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
import numpy as np

data = ['cold', 'cold', 'warm', 'cold', 'hot', 'hot', 'warm', 'cold', 'warm', 'hot']
values = array(data)
print(values)

 
# integer encode
label_encoder = LabelEncoder()
integer_encoded = label_encoder.fit_transform(values)
print(integer_encoded)
# [0 0 2 0 1 1 2 0 2 1]
```



- [x] 从一些字符串分类，到数字分类，到OneHot Vector

##### 从OneHot Vector 到数字分类，到对应的分类

先讲从数字分类到对应的分类

```python
# input :[1,2,3,4,5] output ['cold','...',...]
inverted = label_encoder.inverse_transform(input)
```

具体查看[官网教程](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)



从OneHot vector 到数字分类。以为也有inverse_transform,其实没有。

这里使用两个办法

1. argmax（y,1）,返回1所在位置，即第几
2. tf.nn.in_top_k(train_data,labels,k=1)

---对train_data中的所有行向量，返回最大数的位置==labels？ 返回为boolen变量。

这在神经网络中用来评估一个onehot vector 和一个label的预测准确度。