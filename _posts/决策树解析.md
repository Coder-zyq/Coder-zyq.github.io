# 决策树解析

它可以作为分类算法，也可以作为回归算法，同时也特别适合集成学习比如随机森林


## 算法思想

决策树（decision tree）是一个树结构（可以是二叉树或非二叉树）。
其每个非叶节点表示一个特征属性上的测试，每个分支代表这个特征属性在某个值域上的输出，而每个叶节点存放一个类别。

使用决策树进行决策的过程就是从根节点开始，测试待分类项中相应的特征属性，并按照其值选择输出分支，直到到达叶子节点，将叶子节点存放的类别作为决策结果。

总结来说：

决策树模型核心是下面几部分：

- 结点和有向边组成
- 结点有内部结点和叶结点俩种类型
- 内部结点表示一个特征，叶节点表示一个类

决策树代表实例属性值约束的合取的析取式。从树根到树叶的每一条路径对应一组属性测试的合取，树本身对应这些合取的析取。


--------------------

## 决策树的判定过程
树中从根结点到某一个叶子结点的遍历。每一步如何遍历是由数据各个特征的具体特征属性决定。
![](https://ws1.sinaimg.cn/large/c3af64f1gy1fqe88e41wdj20nx0emq8w.jpg)


## 如何构建决策树
> 决策树的关键步骤是分裂属性

**所谓分裂属性就是在某个节点处按照某一特征属性的不同划分构造不同的分支，其目标是让各个分裂子集尽可能地“纯”。尽可能“纯”就是尽量让一个分裂子集中待分类项属于同一类别。**

判断“纯”的方法不同引出了我们的ID3算法，C4.5算法以及CART算法

评估节点的Impurity 通常有三种办法
- Gini Index（CART）
- Entropy(ID3,C4.5)
- Misclassification error

## ID3算法

### 信息熵基础

### 算法思路以及实例

### 算法的不足


## C4.5算法

### 算法思路

### 存在的优化空间



## CART算法

## 参考
[博客写的很好](https://blog.csdn.net/baimafujinji/article/details/51724371)
